2025-10-22 20:08:06.717 | INFO     | __main__:main:56 - base_lr: 0.0001
batch_size: 32
batch_size_val: 32
clip_pretrain: ./pretrain/ViT-B-16.pt
data_root: ./ln_data
dataset: unc
dim_ffn: 512
dino_layers: 24
dino_name: dinov2-large
dino_pretrain: ./pretrain/dinov2_vitl14_reg4_pretrain.pth
dinov2: True
dinov2_only_backbone: True
dist_backend: nccl
dist_url: tcp://localhost:3682
dropout: 0.1
epochs: 90
evaluate: True
exp_name: L_V14_64_8_512 2025-10-22-20-08-06
input_size: 224
intermediate: False
ladder_dim: 64
lr_decay: 0.1
lr_multi: 1
manual_seed: 0
max_norm: 0.0
milestones: [60]
model_dir: exp/refcoco/L_V14_64_8_512 2025-10-20-12-53-58/best_model.pth
model_name: CLIP-b-16
multiprocessing_distributed: True
nhead: 8
num_head: 8
num_layers: 3
output_dir: exp/refcoco/L_V14_64_8_512 2025-10-22-20-08-06
output_folder: exp/refcoco
print_freq: 100
rank: 0
resume: None
save_freq: 1
split_root: ./ln_data
start_epoch: 0
sync_bn: True
test_split: testB
train_split: train
txt_adapter_dim: 56
txtual_adapter_layer: [1, 3, 5, 7, 9, 11]
val_split: val
vis_dim: 512
vis_dir: exp/refcoco/L_V14_64_8_512 2025-10-22-20-08-06/vis
visual_adapter_dim: 56
visual_adapter_layer: [18, 19, 20, 21, 22, 23]
visualize: True
weight: None
weight_decay: 0.0
word_dim: 512
word_len: 40
workers: 32
workers_val: 16
world_size: 1
2025-10-22 20:08:09.222 | INFO     | model:build_swimvg:26 - Backbone with decay=144, Head=8
2025-10-22 20:08:09.222 | INFO     | model:build_swimvg:36 - number of updated params (Backbone): 7652256.
2025-10-22 20:08:09.222 | INFO     | model:build_swimvg:38 - number of updated params (Head)    : 2104836
2025-10-22 20:08:09.222 | INFO     | model:build_swimvg:40 - number of updated params (neck)    : 0
2025-10-22 20:08:09.222 | INFO     | model:build_swimvg:42 - number of updated params (decoder)    : 0
2025-10-22 20:08:09.222 | INFO     | model:build_swimvg:44 - number of updated params (proj)    : 0
2025-10-22 20:08:09.222 | INFO     | model:build_swimvg:46 - number of fixed params             : 367796737
2025-10-22 20:08:09.408 | INFO     | __main__:main:75 - DataParallel(
  (module): SwimVG(
    (backbone): CLIP(
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (adapter): TextAdapter(
              (D_fc1): Linear(in_features=512, out_features=56, bias=True)
              (D_fc2): Linear(in_features=56, out_features=512, bias=True)
            )
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (adapter): TextAdapter(
              (D_fc1): Linear(in_features=512, out_features=56, bias=True)
              (D_fc2): Linear(in_features=56, out_features=512, bias=True)
            )
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (adapter): TextAdapter(
              (D_fc1): Linear(in_features=512, out_features=56, bias=True)
              (D_fc2): Linear(in_features=56, out_features=512, bias=True)
            )
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (adapter): TextAdapter(
              (D_fc1): Linear(in_features=512, out_features=56, bias=True)
              (D_fc2): Linear(in_features=56, out_features=512, bias=True)
            )
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (adapter): TextAdapter(
              (D_fc1): Linear(in_features=512, out_features=56, bias=True)
              (D_fc2): Linear(in_features=56, out_features=512, bias=True)
            )
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (adapter): TextAdapter(
              (D_fc1): Linear(in_features=512, out_features=56, bias=True)
              (D_fc2): Linear(in_features=56, out_features=512, bias=True)
            )
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (fusion): Fusion()
    (dinov2): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x NestedTensorBlock(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MemEffAttention(
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): LayerScale()
          (drop_path1): Identity()
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ls2): LayerScale()
          (drop_path2): Identity()
          (adapter_proj): Linear(in_features=512, out_features=1024, bias=True)
        )
        (12-17): 6 x NestedTensorBlock(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MemEffAttention(
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): LayerScale()
          (drop_path1): Identity()
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ls2): LayerScale()
          (drop_path2): Identity()
        )
        (18-23): 6 x NestedTensorBlock(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MemEffAttention(
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): LayerScale()
          (drop_path1): Identity()
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ls2): LayerScale()
          (drop_path2): Identity()
          (adapter): Adapter(
            (D_fc1): Linear(in_features=1024, out_features=56, bias=True)
            (D_fc2): Linear(in_features=56, out_features=1024, bias=True)
            (visual): Linear(in_features=56, out_features=56, bias=True)
            (cross): CrossModalAttention(
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=56, out_features=56, bias=True)
              )
              (image_proj): Linear(in_features=56, out_features=56, bias=True)
              (text_proj): Linear(in_features=512, out_features=56, bias=True)
              (back_proj): Linear(in_features=56, out_features=56, bias=True)
            )
          )
          (drop_path): Identity()
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (reg_token): Embedding(1, 1024)
    (txt_token): Embedding(1, 512)
    (bbox_embed): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=1024, out_features=1024, bias=True)
        (2): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
)
2025-10-22 20:08:09.410 | INFO     | __main__:main:80 - => loading checkpoint 'exp/refcoco/L_V14_64_8_512 2025-10-20-12-53-58/best_model.pth'
2025-10-22 20:08:09.703 | INFO     | __main__:main:83 - => loaded checkpoint 'exp/refcoco/L_V14_64_8_512 2025-10-20-12-53-58/best_model.pth'
2025-10-22 20:08:53.758 | INFO     | engine.engine:inference:152 - => Metric Calculation <=
2025-10-22 20:08:53.759 | INFO     | engine.engine:inference:153 - Accu: 83.98.
